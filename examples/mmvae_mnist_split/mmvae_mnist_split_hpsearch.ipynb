{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a MÂ²VAE with a split MNIST data set and evaluate the hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vae_tools.sanity\n",
    "import vae_tools.viz\n",
    "import vae_tools.callbacks\n",
    "import vae_tools.loader\n",
    "from vae_tools.mmvae import MmVae, ReconstructionLoss\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop\n",
    "vae_tools.sanity.check()\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "# Set the seed for reproducible results\n",
    "import vae_tools.sampling\n",
    "vae_tools.sampling.set_seed(0)\n",
    "# resize the notebook if desired\n",
    "#vae_tools.nb_tools.notebook_resize()\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run(seed = 0):\n",
    "\n",
    "    # Get the split MNIST digits\n",
    "    (x_train_a, x_train_b), (x_test_a, x_test_b), y_train, y_test = vae_tools.loader.mnist_split(flatten = True, split = 'hor')\n",
    "\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols, img_chns = 28, 28, 1\n",
    "    original_dim = img_rows * img_cols * img_chns\n",
    "    split_dim = int(original_dim / 2)\n",
    "\n",
    "    # Show a split image\n",
    "\n",
    "    #f, ax = plt.subplots(2,1,sharex=True)\n",
    "    #ax[0].imshow(x_train_a[0,:].reshape(((int(img_rows/2), img_cols))))\n",
    "    #ax[1].imshow(x_train_b[0,:].reshape(((int(img_rows/2), img_cols))))\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "    #%%\n",
    "\n",
    "    p = {'lr': [1.],\n",
    "         'intermediate_dim': [500],\n",
    "         'activation':['tanh'],\n",
    "         'latent_intermediate_dim': [None, 125, 250, 500],\n",
    "         #'latent_activation':['tanh', 'relu', 'elu'],\n",
    "         'latent_activation':['tanh'],\n",
    "         'batch_size': [100],\n",
    "         'epochs': [100],\n",
    "         'optimizer': [RMSprop],\n",
    "         'beta': [1.0],\n",
    "         'beta_mutual': [0.001, 0.01, 0.1, 1.0, 10.],\n",
    "         'reconstruction_loss_metrics': [ReconstructionLoss.BCE],\n",
    "         'z_dim': [20, 40, 80],\n",
    "         'seed': [int(seed)]}\n",
    "\n",
    "\n",
    "    # Define the storage location for the networks\n",
    "    dump_loc = '/mnt/ssd_pcie/mmvae_mnist_split/' + str(p['seed'][0]) + '/'\n",
    "\n",
    "    ## Define the training loop\n",
    "    def hp_process(x_train, y_train, x_val, y_val, params):\n",
    "        # resetting the layer name generation counter\n",
    "        tf.keras.backend.clear_session()\n",
    "        # Build the model and train it\n",
    "        vae_tools.sampling.set_seed(params['seed'])\n",
    "\n",
    "        encoder = [\n",
    "            [\n",
    "                Input(shape=(split_dim,), name=\"input_a\"),\n",
    "                Dense(params['intermediate_dim'], activation=params['activation'], name=\"enc_a\")\n",
    "            ],\n",
    "            [\n",
    "                Input(shape=(split_dim,), name=\"input_b\"),\n",
    "                Dense(params['intermediate_dim'], activation=params['activation'], name=\"enc_b\")\n",
    "            ],\n",
    "        ]\n",
    "\n",
    "        decoder = [\n",
    "            [\n",
    "                Dense(params['intermediate_dim'], activation=params['activation'], name=\"dec_a\"),\n",
    "                Dense(split_dim, activation='sigmoid', name=\"output_a\")\n",
    "            ],\n",
    "            [\n",
    "                Dense(params['intermediate_dim'], activation=params['activation'], name=\"dec_b\"),\n",
    "                Dense(split_dim, activation='sigmoid', name=\"output_b\")\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        le = None\n",
    "        if params['latent_intermediate_dim'] != None:\n",
    "            le = vae_tools.vae.LatentEncoder(layer_dimensions=[params['latent_intermediate_dim']],\n",
    "                                             is_relative=[False],\n",
    "                                             activations=[params['latent_activation']])\n",
    "\n",
    "        vae_obj = MmVae(params['z_dim'], encoder, decoder, [split_dim, split_dim], params['beta'],\n",
    "                        latent_encoder = le, beta_mutual = params['beta_mutual'],\n",
    "                        reconstruction_loss_metrics = [params['reconstruction_loss_metrics']], name='MMVAE')\n",
    "\n",
    "        vae_model = vae_obj.get_model()\n",
    "        vae_model.compile(optimizer=params['optimizer'](vae_tools.sanity.lr_normalizer(params['lr'], params['optimizer'])), loss=None)\n",
    "        #vae_tools.viz.plot_model(vae, file = 'myVAE', print_svg = False, verbose = True)\n",
    "\n",
    "        # Train\n",
    "        h = vae_model.fit(x_train,\n",
    "                    shuffle=True,\n",
    "                    epochs=params['epochs'],\n",
    "                    batch_size=params['batch_size'],\n",
    "                    validation_data=(x_val, None),\n",
    "                    verbose = 2\n",
    "                    )\n",
    "        # Store the final models\n",
    "        vae_obj.store_model_powerset(dump_loc + 'enc_mean_' + str(params['index']) + '_ab_', vae_obj.encoder_inputs, vae_obj.get_encoder_mean)\n",
    "        vae_obj.store_model_powerset(dump_loc + 'enc_logvar_' + str(params['index']) + '_ab_', vae_obj.encoder_inputs, vae_obj.get_encoder_logvar)\n",
    "        vae_obj.get_decoder().save(dump_loc + 'dec_' + str(params['index']) + \"_a.h5\")\n",
    "\n",
    "        return h.history.copy()\n",
    "\n",
    "\n",
    "    ## Hyperparameter (hp) search\n",
    "    # Get all combinations of hp\n",
    "    hp = [dict(zip(p, v)) for v in product(*p.values())]\n",
    "    # add an index to the hyperparameters\n",
    "    for h, idx in zip(hp, list(range(len(hp)))):\n",
    "        h.update({'index': idx})\n",
    "\n",
    "    hp_h = [] # list of histories\n",
    "\n",
    "    # Perform grid search\n",
    "    for params in hp:\n",
    "        h = hp_process([x_train_a, x_train_b], y_train, [x_test_a, x_test_b], y_test, params)\n",
    "        hp_h.append(h)\n",
    "\n",
    "\n",
    "    ## Create a pandas dataframe (df) and store it\n",
    "\n",
    "    # Store just everything into a folder\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    # Prefixes for history and for the full history as a list\n",
    "    h_prefix = 'h_'\n",
    "    h_list_prefix = 'list_'\n",
    "\n",
    "    # init hp keys\n",
    "    for k in hp[0].keys():\n",
    "        data[k] = []\n",
    "    # write hp keys\n",
    "    for params in hp:\n",
    "        for k in params.keys():\n",
    "            data[k].append(params[k])\n",
    "\n",
    "    # init history keys\n",
    "    for k in hp_h[0].keys():\n",
    "        data[h_prefix + k] = []\n",
    "        data[h_prefix + h_list_prefix + k] = []\n",
    "    # write history keys\n",
    "    for h in hp_h:\n",
    "        for k in h.keys():\n",
    "            data[h_prefix + h_list_prefix + k].append(h[k])\n",
    "    # write final history keys\n",
    "    for h in hp_h:\n",
    "        for k in h.keys():\n",
    "            data[h_prefix + k].append(h[k][-1])\n",
    "\n",
    "    # Create pandas dataframe and store it\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_hdf(dump_loc + 'history.h5', key='df')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-24627164",
   "language": "python",
   "display_name": "PyCharm (vae_tools)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 987,
   "position": {
    "height": "40px",
    "left": "1273.52px",
    "right": "20px",
    "top": "255.75px",
    "width": "600px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}