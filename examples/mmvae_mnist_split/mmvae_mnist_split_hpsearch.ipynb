{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a MÂ²VAE with a split MNIST data set and evaluate the hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version:  3.5.2\n",
      "keras version: 2.2.4-tf\n",
      "tensorflow version: 2.0.2\n",
      "matplotlib uses:  module://ipykernel.pylab.backend_inline\n",
      "Available GPUs True\n"
     ]
    }
   ],
   "source": [
    "import vae_tools.sanity\n",
    "import vae_tools.viz\n",
    "import vae_tools.callbacks\n",
    "from vae_tools.mmvae import MmVae, ReconstructionLoss\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop\n",
    "vae_tools.sanity.check()\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "# Set the seed for reproducible results\n",
    "import vae_tools.sampling\n",
    "vae_tools.sampling.set_seed(0)\n",
    "# resize the notebook if desired\n",
    "#vae_tools.nb_tools.notebook_resize()\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO move to loader\n",
    "\n",
    "# Get the MNIST digits\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "# Cut down data set for testing\n",
    "#x_train = x_train[:10,:]\n",
    "#y_train = y_train[:10]\n",
    "#x_test = x_test[:10,:]\n",
    "#y_test = y_test[:10]\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols, img_chns = 28, 28, 1\n",
    "original_dim = img_rows * img_cols * img_chns\n",
    "split_dim = int(original_dim / 2)\n",
    "\n",
    "# Split it horizontally\n",
    "x_train_a = x_train[:,:split_dim]\n",
    "x_train_b = x_train[:,split_dim:]\n",
    "x_test_a = x_test[:,:split_dim]\n",
    "x_test_b = x_test[:,split_dim:]\n",
    "\n",
    "# Show a split image\n",
    "\n",
    "#f, ax = plt.subplots(2,1,sharex=True)\n",
    "#ax[0].imshow(x_train_a[0,:].reshape(((int(img_rows/2), img_cols))))\n",
    "#ax[1].imshow(x_train_b[0,:].reshape(((int(img_rows/2), img_cols))))\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "p = {'lr': [1.],\n",
    "     'intermediate_dim': [500],\n",
    "     'activation':['tanh'],\n",
    "     'latent_intermediate_dim': [None, 125, 250, 500],\n",
    "     #'latent_activation':['tanh', 'relu', 'elu'],\n",
    "     'latent_activation':['tanh'],\n",
    "     'batch_size': [100],\n",
    "     'epochs': [100],\n",
    "     'optimizer': [RMSprop],\n",
    "     'beta': [1.0],\n",
    "     'beta_mutual': [0.001, 0.01, 0.1, 1.0, 10.],\n",
    "     'reconstruction_loss_metrics': [ReconstructionLoss.BCE],\n",
    "     'z_dim': [20, 40, 80],\n",
    "     'seed': [0,1,2,3,4]}\n",
    "\n",
    "dump_loc = '/mnt/ssd_pcie/mmvae_mnist_split/' + str(p['seed'][0]) + '/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hp_process(x_train, y_train, x_val, y_val, params):\n",
    "    # resetting the layer name generation counter\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Build the model and train it\n",
    "    vae_tools.sampling.set_seed(params['seed'])\n",
    "\n",
    "    encoder = [\n",
    "        [\n",
    "            Input(shape=(split_dim,), name=\"input_a\"),\n",
    "            Dense(params['intermediate_dim'], activation=params['activation'], name=\"enc_a\")\n",
    "        ],\n",
    "        [\n",
    "            Input(shape=(split_dim,), name=\"input_b\"),\n",
    "            Dense(params['intermediate_dim'], activation=params['activation'], name=\"enc_b\")\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    decoder = [\n",
    "        [\n",
    "            Dense(params['intermediate_dim'], activation=params['activation'], name=\"dec_a\"),\n",
    "            Dense(split_dim, activation='sigmoid', name=\"output_a\")\n",
    "        ],\n",
    "        [\n",
    "            Dense(params['intermediate_dim'], activation=params['activation'], name=\"dec_b\"),\n",
    "            Dense(split_dim, activation='sigmoid', name=\"output_b\")\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    le = None\n",
    "    if params['latent_intermediate_dim'] != None:\n",
    "        le = vae_tools.vae.LatentEncoder(layer_dimensions=[params['latent_intermediate_dim']],\n",
    "                                         is_relative=[False],\n",
    "                                         activations=[params['latent_activation']])\n",
    "\n",
    "    vae_obj = MmVae(params['z_dim'], encoder, decoder, [split_dim, split_dim], params['beta'],\n",
    "                    latent_encoder = le, beta_mutual = params['beta'],\n",
    "                    reconstruction_loss_metrics = [params['reconstruction_loss_metrics']], name='MMVAE')\n",
    "\n",
    "    vae_model = vae_obj.get_model()\n",
    "    vae_model.compile(optimizer=params['optimizer'](vae_tools.sanity.lr_normalizer(params['lr'], params['optimizer'])), loss=None)\n",
    "    #vae_tools.viz.plot_model(vae, file = 'myVAE', print_svg = False, verbose = True)\n",
    "\n",
    "    # Train\n",
    "    h = vae_model.fit(x_train,\n",
    "                shuffle=True,\n",
    "                epochs=params['epochs'],\n",
    "                batch_size=params['batch_size'],\n",
    "                validation_data=(x_val, None),\n",
    "                verbose = 2\n",
    "                )\n",
    "    # Store the final models\n",
    "    vae_obj.store_model_powerset(dump_loc + 'enc_mean_' + str(params['index']) + '_ab_', vae_obj.encoder_inputs, vae_obj.get_encoder_mean)\n",
    "    vae_obj.store_model_powerset(dump_loc + 'enc_logvar_' + str(params['index']) + '_ab_', vae_obj.encoder_inputs, vae_obj.get_encoder_logvar)\n",
    "    vae_obj.get_decoder().save(dump_loc + 'dec_' + str(params['index']) + \"_a.h5\")\n",
    "\n",
    "    return h.history.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter (hp) search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output loss_reconstruction_0_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_0_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_1_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_1_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_1.\n",
      "WARNING:tensorflow:Output loss_prior_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_0.\n",
      "WARNING:tensorflow:Output loss_prior_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_1.\n",
      "WARNING:tensorflow:Output loss_prior_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_2.\n",
      "WARNING:tensorflow:Output loss_mutual_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_0.\n",
      "WARNING:tensorflow:Output loss_mutual_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_1.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "60000/60000 - 8s - loss: 307.1934 - loss_reconstruction_0_0: 59.9527 - loss_reconstruction_1_0: 65.8105 - loss_reconstruction_2_0: 63.6311 - loss_reconstruction_2_1: 69.7265 - loss_prior_0: 10.5669 - loss_prior_1: 11.2171 - loss_prior_2: 13.8581 - loss_mutual_0: 6.4292 - loss_mutual_1: 6.0012 - val_loss: 251.1212 - val_loss_reconstruction_0_0: 45.6947 - val_loss_reconstruction_1_0: 50.1422 - val_loss_reconstruction_2_0: 50.0233 - val_loss_reconstruction_2_1: 57.0657 - val_loss_prior_0: 10.7938 - val_loss_prior_1: 11.4484 - val_loss_prior_2: 13.8374 - val_loss_mutual_0: 6.3463 - val_loss_mutual_1: 5.7694\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_0_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_0_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_0_a_11.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_0_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_0_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_0_a_11.h5\n",
      "WARNING:tensorflow:Output loss_reconstruction_0_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_0_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_1_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_1_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_1.\n",
      "WARNING:tensorflow:Output loss_prior_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_0.\n",
      "WARNING:tensorflow:Output loss_prior_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_1.\n",
      "WARNING:tensorflow:Output loss_prior_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_2.\n",
      "WARNING:tensorflow:Output loss_mutual_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_0.\n",
      "WARNING:tensorflow:Output loss_mutual_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_1.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "60000/60000 - 8s - loss: 307.2710 - loss_reconstruction_0_0: 60.2004 - loss_reconstruction_1_0: 65.7152 - loss_reconstruction_2_0: 63.6162 - loss_reconstruction_2_1: 69.6674 - loss_prior_0: 10.5811 - loss_prior_1: 11.2171 - loss_prior_2: 13.8823 - loss_mutual_0: 6.4150 - loss_mutual_1: 5.9764 - val_loss: 250.3727 - val_loss_reconstruction_0_0: 45.5532 - val_loss_reconstruction_1_0: 49.3818 - val_loss_reconstruction_2_0: 50.8912 - val_loss_reconstruction_2_1: 55.8624 - val_loss_prior_0: 10.7092 - val_loss_prior_1: 11.3584 - val_loss_prior_2: 14.1315 - val_loss_mutual_0: 6.4629 - val_loss_mutual_1: 6.0221\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_1_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_1_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_1_a_11.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_1_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_1_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_1_a_11.h5\n",
      "WARNING:tensorflow:Output loss_reconstruction_0_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_0_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_1_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_1_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_1.\n",
      "WARNING:tensorflow:Output loss_prior_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_0.\n",
      "WARNING:tensorflow:Output loss_prior_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_1.\n",
      "WARNING:tensorflow:Output loss_prior_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_2.\n",
      "WARNING:tensorflow:Output loss_mutual_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_0.\n",
      "WARNING:tensorflow:Output loss_mutual_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_1.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "60000/60000 - 8s - loss: 307.2687 - loss_reconstruction_0_0: 60.3575 - loss_reconstruction_1_0: 65.5113 - loss_reconstruction_2_0: 63.8185 - loss_reconstruction_2_1: 69.5353 - loss_prior_0: 10.5416 - loss_prior_1: 11.2316 - loss_prior_2: 13.8983 - loss_mutual_0: 6.4332 - loss_mutual_1: 5.9415 - val_loss: 250.2597 - val_loss_reconstruction_0_0: 46.0995 - val_loss_reconstruction_1_0: 49.1259 - val_loss_reconstruction_2_0: 50.4059 - val_loss_reconstruction_2_1: 56.2169 - val_loss_prior_0: 10.7624 - val_loss_prior_1: 11.1616 - val_loss_prior_2: 13.5838 - val_loss_mutual_0: 6.8064 - val_loss_mutual_1: 6.0973\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_2_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_2_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_2_a_11.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_2_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_2_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_2_a_11.h5\n",
      "WARNING:tensorflow:Output loss_reconstruction_0_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_0_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_1_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_1_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_1.\n",
      "WARNING:tensorflow:Output loss_prior_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_0.\n",
      "WARNING:tensorflow:Output loss_prior_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_1.\n",
      "WARNING:tensorflow:Output loss_prior_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_2.\n",
      "WARNING:tensorflow:Output loss_mutual_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_0.\n",
      "WARNING:tensorflow:Output loss_mutual_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_1.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "60000/60000 - 8s - loss: 307.2004 - loss_reconstruction_0_0: 59.9426 - loss_reconstruction_1_0: 65.7986 - loss_reconstruction_2_0: 63.6412 - loss_reconstruction_2_1: 69.7404 - loss_prior_0: 10.5643 - loss_prior_1: 11.2178 - loss_prior_2: 13.8680 - loss_mutual_0: 6.4331 - loss_mutual_1: 5.9944 - val_loss: 250.1927 - val_loss_reconstruction_0_0: 44.7800 - val_loss_reconstruction_1_0: 49.4590 - val_loss_reconstruction_2_0: 50.9500 - val_loss_reconstruction_2_1: 56.7617 - val_loss_prior_0: 10.9041 - val_loss_prior_1: 11.4653 - val_loss_prior_2: 13.4400 - val_loss_mutual_0: 6.3551 - val_loss_mutual_1: 6.0776\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_3_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_3_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_3_a_11.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_3_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_3_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_3_a_11.h5\n",
      "WARNING:tensorflow:Output loss_reconstruction_0_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_0_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_1_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_1_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_1.\n",
      "WARNING:tensorflow:Output loss_prior_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_0.\n",
      "WARNING:tensorflow:Output loss_prior_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_1.\n",
      "WARNING:tensorflow:Output loss_prior_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_2.\n",
      "WARNING:tensorflow:Output loss_mutual_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_0.\n",
      "WARNING:tensorflow:Output loss_mutual_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_1.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "60000/60000 - 8s - loss: 307.4460 - loss_reconstruction_0_0: 60.3439 - loss_reconstruction_1_0: 65.7767 - loss_reconstruction_2_0: 63.7654 - loss_reconstruction_2_1: 69.5928 - loss_prior_0: 10.5131 - loss_prior_1: 11.2424 - loss_prior_2: 13.8625 - loss_mutual_0: 6.4498 - loss_mutual_1: 5.8993 - val_loss: 248.9235 - val_loss_reconstruction_0_0: 45.1231 - val_loss_reconstruction_1_0: 50.8924 - val_loss_reconstruction_2_0: 50.0725 - val_loss_reconstruction_2_1: 55.0640 - val_loss_prior_0: 10.6916 - val_loss_prior_1: 11.1819 - val_loss_prior_2: 13.7352 - val_loss_mutual_0: 6.3973 - val_loss_mutual_1: 5.7655\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_4_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_4_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_4_a_11.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_4_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_4_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_4_a_11.h5\n",
      "WARNING:tensorflow:Output loss_reconstruction_0_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_0_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_1_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_1_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_1.\n",
      "WARNING:tensorflow:Output loss_prior_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_0.\n",
      "WARNING:tensorflow:Output loss_prior_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_1.\n",
      "WARNING:tensorflow:Output loss_prior_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_2.\n",
      "WARNING:tensorflow:Output loss_mutual_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_0.\n",
      "WARNING:tensorflow:Output loss_mutual_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_1.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "60000/60000 - 8s - loss: 311.4420 - loss_reconstruction_0_0: 59.1759 - loss_reconstruction_1_0: 64.8577 - loss_reconstruction_2_0: 61.2193 - loss_reconstruction_2_1: 67.2674 - loss_prior_0: 13.0652 - loss_prior_1: 13.9811 - loss_prior_2: 17.7205 - loss_mutual_0: 7.2934 - loss_mutual_1: 6.8613 - val_loss: 247.3840 - val_loss_reconstruction_0_0: 42.9234 - val_loss_reconstruction_1_0: 46.9098 - val_loss_reconstruction_2_0: 45.0519 - val_loss_reconstruction_2_1: 48.8306 - val_loss_prior_0: 13.6373 - val_loss_prior_1: 15.0496 - val_loss_prior_2: 19.3841 - val_loss_mutual_0: 8.2891 - val_loss_mutual_1: 7.3082\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_5_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_5_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_5_a_11.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_5_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_5_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_5_a_11.h5\n",
      "WARNING:tensorflow:Output loss_reconstruction_0_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_0_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_1_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_1_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_1.\n",
      "WARNING:tensorflow:Output loss_prior_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_0.\n",
      "WARNING:tensorflow:Output loss_prior_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_1.\n",
      "WARNING:tensorflow:Output loss_prior_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_2.\n",
      "WARNING:tensorflow:Output loss_mutual_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_0.\n",
      "WARNING:tensorflow:Output loss_mutual_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_1.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "60000/60000 - 8s - loss: 311.5003 - loss_reconstruction_0_0: 59.1728 - loss_reconstruction_1_0: 64.8441 - loss_reconstruction_2_0: 61.2549 - loss_reconstruction_2_1: 67.3096 - loss_prior_0: 13.0704 - loss_prior_1: 13.9758 - loss_prior_2: 17.7255 - loss_mutual_0: 7.2937 - loss_mutual_1: 6.8535 - val_loss: 250.5806 - val_loss_reconstruction_0_0: 43.1670 - val_loss_reconstruction_1_0: 46.8835 - val_loss_reconstruction_2_0: 45.5830 - val_loss_reconstruction_2_1: 50.3036 - val_loss_prior_0: 13.6105 - val_loss_prior_1: 15.0819 - val_loss_prior_2: 19.4533 - val_loss_mutual_0: 8.4649 - val_loss_mutual_1: 8.0329\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_6_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_6_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_6_a_11.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_6_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_6_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_6_a_11.h5\n",
      "WARNING:tensorflow:Output loss_reconstruction_0_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_0_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_1_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_1_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_1.\n",
      "WARNING:tensorflow:Output loss_prior_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_0.\n",
      "WARNING:tensorflow:Output loss_prior_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_1.\n",
      "WARNING:tensorflow:Output loss_prior_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_2.\n",
      "WARNING:tensorflow:Output loss_mutual_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_0.\n",
      "WARNING:tensorflow:Output loss_mutual_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_1.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "60000/60000 - 8s - loss: 311.4569 - loss_reconstruction_0_0: 59.1796 - loss_reconstruction_1_0: 64.8442 - loss_reconstruction_2_0: 61.2546 - loss_reconstruction_2_1: 67.2887 - loss_prior_0: 13.0624 - loss_prior_1: 13.9726 - loss_prior_2: 17.7167 - loss_mutual_0: 7.2860 - loss_mutual_1: 6.8522 - val_loss: 248.3337 - val_loss_reconstruction_0_0: 43.3970 - val_loss_reconstruction_1_0: 46.7774 - val_loss_reconstruction_2_0: 45.0360 - val_loss_reconstruction_2_1: 49.3995 - val_loss_prior_0: 13.7531 - val_loss_prior_1: 14.8919 - val_loss_prior_2: 19.5358 - val_loss_mutual_0: 8.3178 - val_loss_mutual_1: 7.2252\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_7_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_7_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_7_a_11.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_7_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_7_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_7_a_11.h5\n",
      "WARNING:tensorflow:Output loss_reconstruction_0_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_0_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_1_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_1_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_1.\n",
      "WARNING:tensorflow:Output loss_prior_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_0.\n",
      "WARNING:tensorflow:Output loss_prior_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_1.\n",
      "WARNING:tensorflow:Output loss_prior_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_2.\n",
      "WARNING:tensorflow:Output loss_mutual_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_0.\n",
      "WARNING:tensorflow:Output loss_mutual_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_1.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "60000/60000 - 8s - loss: 311.5555 - loss_reconstruction_0_0: 59.1926 - loss_reconstruction_1_0: 64.8610 - loss_reconstruction_2_0: 61.2676 - loss_reconstruction_2_1: 67.3182 - loss_prior_0: 13.0708 - loss_prior_1: 13.9799 - loss_prior_2: 17.7320 - loss_mutual_0: 7.2841 - loss_mutual_1: 6.8492 - val_loss: 249.1509 - val_loss_reconstruction_0_0: 43.3060 - val_loss_reconstruction_1_0: 47.1674 - val_loss_reconstruction_2_0: 45.0889 - val_loss_reconstruction_2_1: 49.3435 - val_loss_prior_0: 13.6993 - val_loss_prior_1: 15.0388 - val_loss_prior_2: 19.3733 - val_loss_mutual_0: 8.2761 - val_loss_mutual_1: 7.8576\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_8_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_8_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_8_a_11.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_8_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_8_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_8_a_11.h5\n",
      "WARNING:tensorflow:Output loss_reconstruction_0_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_0_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_1_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_1_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_1.\n",
      "WARNING:tensorflow:Output loss_prior_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_0.\n",
      "WARNING:tensorflow:Output loss_prior_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_1.\n",
      "WARNING:tensorflow:Output loss_prior_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_2.\n",
      "WARNING:tensorflow:Output loss_mutual_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_0.\n",
      "WARNING:tensorflow:Output loss_mutual_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_1.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "60000/60000 - 8s - loss: 311.6017 - loss_reconstruction_0_0: 59.1930 - loss_reconstruction_1_0: 64.8644 - loss_reconstruction_2_0: 61.2774 - loss_reconstruction_2_1: 67.3240 - loss_prior_0: 13.0687 - loss_prior_1: 13.9872 - loss_prior_2: 17.7353 - loss_mutual_0: 7.2930 - loss_mutual_1: 6.8585 - val_loss: 251.2325 - val_loss_reconstruction_0_0: 43.9054 - val_loss_reconstruction_1_0: 47.0400 - val_loss_reconstruction_2_0: 45.3730 - val_loss_reconstruction_2_1: 50.1861 - val_loss_prior_0: 13.7352 - val_loss_prior_1: 15.1280 - val_loss_prior_2: 19.6748 - val_loss_mutual_0: 8.4255 - val_loss_mutual_1: 7.7644\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_9_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_9_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_mean_9_a_11.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_9_a_10.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_9_a_01.h5\n",
      "Saved model and weights to disk: /mnt/ssd_pcie/mmvae_mnist_split/enc_logvar_9_a_11.h5\n",
      "WARNING:tensorflow:Output loss_reconstruction_0_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_0_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_1_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_1_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_1.\n",
      "WARNING:tensorflow:Output loss_prior_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_0.\n",
      "WARNING:tensorflow:Output loss_prior_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_1.\n",
      "WARNING:tensorflow:Output loss_prior_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_2.\n",
      "WARNING:tensorflow:Output loss_mutual_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_0.\n",
      "WARNING:tensorflow:Output loss_mutual_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_1.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-1deeac554cb7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;31m# Perform grid search\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mparams\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mhp\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m     \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhp_process\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mx_train_a\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx_train_b\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mx_test_a\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx_test_b\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m     \u001B[0mhp_h\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mh\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-9-1ab8ad81d3cd>\u001B[0m in \u001B[0;36mhp_process\u001B[0;34m(x_train, y_train, x_val, y_val, params)\u001B[0m\n\u001B[1;32m     48\u001B[0m                 \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'batch_size'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m                 \u001B[0mvalidation_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_val\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 50\u001B[0;31m                 \u001B[0mverbose\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     51\u001B[0m                 )\n\u001B[1;32m     52\u001B[0m     \u001B[0;31m# Store the final models\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001B[0m\n\u001B[1;32m    726\u001B[0m         \u001B[0mmax_queue_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmax_queue_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    727\u001B[0m         \u001B[0mworkers\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mworkers\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 728\u001B[0;31m         use_multiprocessing=use_multiprocessing)\n\u001B[0m\u001B[1;32m    729\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    730\u001B[0m   def evaluate(self,\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001B[0m\n\u001B[1;32m    322\u001B[0m                 \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mModeKeys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTRAIN\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    323\u001B[0m                 \u001B[0mtraining_context\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtraining_context\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 324\u001B[0;31m                 total_epochs=epochs)\n\u001B[0m\u001B[1;32m    325\u001B[0m             \u001B[0mcbks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake_logs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch_logs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining_result\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mModeKeys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTRAIN\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    326\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001B[0m in \u001B[0;36mrun_one_epoch\u001B[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001B[0m\n\u001B[1;32m    121\u001B[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001B[1;32m    122\u001B[0m       \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m         \u001B[0mbatch_outs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mexecution_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m       \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mStopIteration\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOutOfRangeError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m         \u001B[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001B[0m in \u001B[0;36mexecution_function\u001B[0;34m(input_fn)\u001B[0m\n\u001B[1;32m     84\u001B[0m     \u001B[0;31m# `numpy` translates Tensors to values in Eager mode.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     85\u001B[0m     return nest.map_structure(_non_none_constant_value,\n\u001B[0;32m---> 86\u001B[0;31m                               distributed_function(input_fn))\n\u001B[0m\u001B[1;32m     87\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     88\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mexecution_function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    456\u001B[0m     \u001B[0mtracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 457\u001B[0;31m     \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    458\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mtracing_count\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    459\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_counter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcalled_without_tracing\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    485\u001B[0m       \u001B[0;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    486\u001B[0m       \u001B[0;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 487\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    488\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    489\u001B[0m       \u001B[0;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1821\u001B[0m     \u001B[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1822\u001B[0m     \u001B[0mgraph_function\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1823\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_filtered_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1824\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1825\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\u001B[0m in \u001B[0;36m_filtered_call\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m   1139\u001B[0m          if isinstance(t, (ops.Tensor,\n\u001B[1;32m   1140\u001B[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001B[0;32m-> 1141\u001B[0;31m         self.captured_inputs)\n\u001B[0m\u001B[1;32m   1142\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1143\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_call_flat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcaptured_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcancellation_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1222\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mexecuting_eagerly\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1223\u001B[0m       flat_outputs = forward_function.call(\n\u001B[0;32m-> 1224\u001B[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001B[0m\u001B[1;32m   1225\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1226\u001B[0m       \u001B[0mgradient_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_delayed_rewrite_functions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mregister\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    509\u001B[0m               \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    510\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"executor_type\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexecutor_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"config_proto\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 511\u001B[0;31m               ctx=ctx)\n\u001B[0m\u001B[1;32m    512\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    513\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     59\u001B[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001B[1;32m     60\u001B[0m                                                \u001B[0mop_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattrs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 61\u001B[0;31m                                                num_outputs)\n\u001B[0m\u001B[1;32m     62\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Get all combinations of hp\n",
    "hp = [dict(zip(p, v)) for v in product(*p.values())]\n",
    "# add an index to the hyperparameters\n",
    "for h, idx in zip(hp, list(range(len(hp)))):\n",
    "    h.update({'index': idx})\n",
    "\n",
    "hp_h = [] # list of histories\n",
    "\n",
    "# Perform grid search\n",
    "for params in hp:\n",
    "    h = hp_process([x_train_a, x_train_b], y_train, [x_test_a, x_test_b], y_test, params)\n",
    "    hp_h.append(h)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a pandas dataframe (df) and store it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Store just everything into a folder\n",
    "\n",
    "data = {}\n",
    "\n",
    "# Prefixes for history and for the full history as a list\n",
    "h_prefix = 'h_'\n",
    "h_list_prefix = 'list_'\n",
    "\n",
    "# init hp keys\n",
    "for k in hp[0].keys():\n",
    "    data[k] = []\n",
    "# write hp keys\n",
    "for params in hp:\n",
    "    for k in params.keys():\n",
    "        data[k].append(params[k])\n",
    "\n",
    "# init history keys\n",
    "for k in hp_h[0].keys():\n",
    "    data[h_prefix + k] = []\n",
    "    data[h_prefix + h_list_prefix + k] = []\n",
    "# write history keys\n",
    "for h in hp_h:\n",
    "    for k in h.keys():\n",
    "        data[h_prefix + h_list_prefix + k].append(h[k])\n",
    "# write final history keys\n",
    "for h in hp_h:\n",
    "    for k in h.keys():\n",
    "        data[h_prefix + k].append(h[k][-1])\n",
    "\n",
    "# Create pandas dataframe and store it\n",
    "df = pd.DataFrame(data)\n",
    "df.to_hdf(dump_loc + 'history.h5', key='df')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 987,
   "position": {
    "height": "40px",
    "left": "1273.52px",
    "right": "20px",
    "top": "255.75px",
    "width": "600px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}