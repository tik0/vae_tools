{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a MÂ²VAE with a split MNIST data set and evaluate the hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version:  3.5.2\n",
      "keras version: 2.2.4-tf\n",
      "tensorflow version: 2.0.2\n",
      "matplotlib uses:  module://ipykernel.pylab.backend_inline\n",
      "No GPUs available\n"
     ]
    }
   ],
   "source": [
    "import vae_tools.sanity\n",
    "import vae_tools.viz\n",
    "import vae_tools.callbacks\n",
    "import vae_tools.loader\n",
    "from vae_tools.mmvae import MmVae, ReconstructionLoss\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop\n",
    "vae_tools.sanity.check()\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "# Set the seed for reproducible results\n",
    "import vae_tools.sampling\n",
    "vae_tools.sampling.set_seed(0)\n",
    "# resize the notebook if desired\n",
    "#vae_tools.nb_tools.notebook_resize()\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output loss_reconstruction_0_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_0_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_1_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_1_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_0.\n",
      "WARNING:tensorflow:Output loss_reconstruction_2_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_reconstruction_2_1.\n",
      "WARNING:tensorflow:Output loss_prior_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_0.\n",
      "WARNING:tensorflow:Output loss_prior_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_1.\n",
      "WARNING:tensorflow:Output loss_prior_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_prior_2.\n",
      "WARNING:tensorflow:Output loss_mutual_0 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_0.\n",
      "WARNING:tensorflow:Output loss_mutual_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to loss_mutual_1.\n",
      "Train on 10 samples, validate on 10 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss.\n",
      "10/10 - 3s - loss: 1112.3313 - loss_reconstruction_0_0: 274.8330 - loss_reconstruction_1_0: 280.5603 - loss_reconstruction_2_0: 276.5545 - loss_reconstruction_2_1: 273.7543 - loss_prior_0: 1.6796 - loss_prior_1: 2.5866 - loss_prior_2: 2.3533 - loss_mutual_0: 0.0048 - loss_mutual_1: 0.0050 - val_loss: 1092.6062 - val_loss_reconstruction_0_0: 272.1688 - val_loss_reconstruction_1_0: 268.4044 - val_loss_reconstruction_2_0: 266.4499 - val_loss_reconstruction_2_1: 274.4526 - val_loss_prior_0: 2.5793 - val_loss_prior_1: 2.9332 - val_loss_prior_2: 5.6025 - val_loss_mutual_0: 0.0078 - val_loss_mutual_1: 0.0077\n",
      "Epoch 2/2\n",
      "10/10 - 0s - loss: 1099.5500 - loss_reconstruction_0_0: 272.6115 - loss_reconstruction_1_0: 268.1393 - loss_reconstruction_2_0: 269.1646 - loss_reconstruction_2_1: 277.3934 - loss_prior_0: 2.6363 - loss_prior_1: 2.8535 - loss_prior_2: 6.7363 - loss_mutual_0: 0.0079 - loss_mutual_1: 0.0071 - val_loss: 954.4225 - val_loss_reconstruction_0_0: 237.5743 - val_loss_reconstruction_1_0: 245.0158 - val_loss_reconstruction_2_0: 220.0129 - val_loss_reconstruction_2_1: 224.2288 - val_loss_prior_0: 5.5526 - val_loss_prior_1: 6.0019 - val_loss_prior_2: 16.0109 - val_loss_mutual_0: 0.0131 - val_loss_mutual_1: 0.0122\n",
      "Saved model and weights to disk: /tmp/enc_mean_0_ab_10.h5\n",
      "Saved model and weights to disk: /tmp/enc_mean_0_ab_01.h5\n",
      "Saved model and weights to disk: /tmp/enc_mean_0_ab_11.h5\n",
      "Saved model and weights to disk: /tmp/enc_logvar_0_ab_10.h5\n",
      "Saved model and weights to disk: /tmp/enc_logvar_0_ab_01.h5\n",
      "Saved model and weights to disk: /tmp/enc_logvar_0_ab_11.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twbadmin/.local/lib/python3.5/site-packages/pandas/core/generic.py:2378: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->['activation', 'h_list_loss', 'h_list_loss_mutual_0', 'h_list_loss_mutual_1', 'h_list_loss_prior_0', 'h_list_loss_prior_1', 'h_list_loss_prior_2', 'h_list_loss_reconstruction_0_0', 'h_list_loss_reconstruction_1_0', 'h_list_loss_reconstruction_2_0', 'h_list_loss_reconstruction_2_1', 'h_list_val_loss', 'h_list_val_loss_mutual_0', 'h_list_val_loss_mutual_1', 'h_list_val_loss_prior_0', 'h_list_val_loss_prior_1', 'h_list_val_loss_prior_2', 'h_list_val_loss_reconstruction_0_0', 'h_list_val_loss_reconstruction_1_0', 'h_list_val_loss_reconstruction_2_0', 'h_list_val_loss_reconstruction_2_1', 'latent_activation', 'latent_intermediate_dim', 'optimizer', 'reconstruction_loss_metrics']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def run(seed = 0, loc = '/mnt/ssd_pcie/mmvae_mnist_split/'):\n",
    "\n",
    "    # Get the split MNIST digits\n",
    "    (x_train_a, x_train_b), (x_test_a, x_test_b), y_train, y_test = vae_tools.loader.mnist_split(flatten = True, split = 'hor')\n",
    "\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols, img_chns = 28, 28, 1\n",
    "    original_dim = img_rows * img_cols * img_chns\n",
    "    split_dim = int(original_dim / 2)\n",
    "\n",
    "    # x_train_a = x_train_a[:10]\n",
    "    # x_train_b = x_train_b[:10]\n",
    "    # x_test_a = x_test_a[:10]\n",
    "    # x_test_b = x_test_b[:10]\n",
    "    # x_train_a = x_train_a[:10]\n",
    "    # y_train = y_train[:10]\n",
    "    # y_test = x_train_a[:10]\n",
    "\n",
    "    # Show a split image\n",
    "\n",
    "    #f, ax = plt.subplots(2,1,sharex=True)\n",
    "    #ax[0].imshow(x_train_a[0,:].reshape(((int(img_rows/2), img_cols))))\n",
    "    #ax[1].imshow(x_train_b[0,:].reshape(((int(img_rows/2), img_cols))))\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "    #%%\n",
    "    # 1\n",
    "    p = {'lr': [1.],\n",
    "         'intermediate_dim': [500],\n",
    "         'activation':['tanh'],\n",
    "         'latent_intermediate_dim': [None, 125, 250, 500],\n",
    "         #'latent_activation':['tanh', 'relu', 'elu'],\n",
    "         'latent_activation':['tanh'],\n",
    "         'batch_size': [100],\n",
    "         'epochs': [100],\n",
    "         'optimizer': [RMSprop],\n",
    "         'beta': [1.0],\n",
    "         'beta_mutual': [0.001, 0.01, 0.1, 1.0, 10.],\n",
    "         'reconstruction_loss_metrics': [ReconstructionLoss.BCE],\n",
    "         'z_dim': [20, 40, 80],\n",
    "         'seed': [int(seed)]}\n",
    "    # 2\n",
    "    p = {'lr': [1.],\n",
    "         'intermediate_dim': [500],\n",
    "         'activation':['tanh'],\n",
    "         'latent_intermediate_dim': [None],\n",
    "         #'latent_activation':['tanh', 'relu', 'elu'],\n",
    "         'latent_activation':['tanh'],\n",
    "         'batch_size': [100],\n",
    "         'epochs': [100],\n",
    "         'optimizer': [RMSprop],\n",
    "         'beta': [1.0],\n",
    "         'beta_mutual': [10., 20., 30.],\n",
    "         'reconstruction_loss_metrics': [ReconstructionLoss.BCE],\n",
    "         'z_dim': [2, 5, 10, 15, 20],\n",
    "         'seed': [int(seed)]}\n",
    "    # 3\n",
    "    p = {'lr': [1.],\n",
    "         'intermediate_dim': [500],\n",
    "         'activation':['relu'],\n",
    "         'latent_intermediate_dim': [None],\n",
    "         #'latent_activation':['tanh', 'relu', 'elu'],\n",
    "         'latent_activation':['tanh'],\n",
    "         'batch_size': [512],\n",
    "         'epochs': [100],\n",
    "         'optimizer': [Adam],\n",
    "         'beta': [1.0],\n",
    "         'beta_mutual': [0.001, 0.01, 0.1, 1.0, 10., 20., 30.],\n",
    "         'reconstruction_loss_metrics': [ReconstructionLoss.BCE],\n",
    "         'z_dim': [2, 5, 10, 15, 20, 40, 80],\n",
    "         'seed': [int(seed)]}\n",
    "    # 4\n",
    "    p = {'lr': [1.],\n",
    "         'intermediate_dim': [500],\n",
    "         'activation':['relu'],\n",
    "         'latent_intermediate_dim': [None],\n",
    "         #'latent_activation':['tanh', 'relu', 'elu'],\n",
    "         'latent_activation':['tanh'],\n",
    "         'batch_size': [512],\n",
    "         'epochs': [100],\n",
    "         'optimizer': [Adam],\n",
    "         'beta': [1.0],\n",
    "         'beta_mutual': [0.001, 0.01, 0.1, 1.0, 10., 20., 30.],\n",
    "         'reconstruction_loss_metrics': [ReconstructionLoss.BCE],\n",
    "         'z_dim': [2, 5, 10, 15, 20, 40, 80],\n",
    "         'seed': [int(seed)],\n",
    "         'shared_weights': [False]}\n",
    "\n",
    "\n",
    "    # Define the storage location for the networks\n",
    "    dump_loc = loc + str(p['seed'][0]) + '/'\n",
    "\n",
    "    ## Define the training loop\n",
    "    def hp_process(x_train, y_train, x_val, y_val, params):\n",
    "        # resetting the layer name generation counter\n",
    "        tf.keras.backend.clear_session()\n",
    "        # Build the model and train it\n",
    "        vae_tools.sampling.set_seed(params['seed'])\n",
    "\n",
    "        encoder = [\n",
    "            [\n",
    "                Input(shape=(split_dim,), name=\"input_a\"),\n",
    "                Dense(params['intermediate_dim'], activation=params['activation'], name=\"enc_a\")\n",
    "            ],\n",
    "            [\n",
    "                Input(shape=(split_dim,), name=\"input_b\"),\n",
    "                Dense(params['intermediate_dim'], activation=params['activation'], name=\"enc_b\")\n",
    "            ],\n",
    "        ]\n",
    "\n",
    "        decoder = [\n",
    "            [\n",
    "                Dense(params['intermediate_dim'], activation=params['activation'], name=\"dec_a\"),\n",
    "                Dense(split_dim, activation='sigmoid', name=\"output_a\")\n",
    "            ],\n",
    "            [\n",
    "                Dense(params['intermediate_dim'], activation=params['activation'], name=\"dec_b\"),\n",
    "                Dense(split_dim, activation='sigmoid', name=\"output_b\")\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        le = None\n",
    "        if params['latent_intermediate_dim'] != None:\n",
    "            le = vae_tools.vae.LatentEncoder(layer_dimensions=[params['latent_intermediate_dim']],\n",
    "                                             is_relative=[False],\n",
    "                                             activations=[params['latent_activation']])\n",
    "\n",
    "        vae_obj = MmVae(params['z_dim'], encoder, decoder, [split_dim, split_dim], params['beta'],\n",
    "                        latent_encoder = le, beta_mutual = params['beta_mutual'],\n",
    "                        reconstruction_loss_metrics = [params['reconstruction_loss_metrics']],\n",
    "                        shared_weights=params['shared_weights'], name='MMVAE')\n",
    "\n",
    "        vae_model = vae_obj.get_model()\n",
    "        vae_model.compile(optimizer=params['optimizer'](vae_tools.sanity.lr_normalizer(params['lr'], params['optimizer'])), loss=None)\n",
    "        #vae_tools.viz.plot_model(vae, file = 'myVAE', print_svg = False, verbose = True)\n",
    "\n",
    "        # Train\n",
    "        h = vae_model.fit(x_train,\n",
    "                    shuffle=True,\n",
    "                    epochs=params['epochs'],\n",
    "                    batch_size=params['batch_size'],\n",
    "                    validation_data=(x_val, None),\n",
    "                    verbose = 2\n",
    "                    )\n",
    "        # Store the final models\n",
    "        vae_obj.store_model_powerset(dump_loc + 'enc_mean_' + str(params['index']) + '_ab_', vae_obj.encoder_inputs, vae_obj.get_encoder_mean)\n",
    "        vae_obj.store_model_powerset(dump_loc + 'enc_logvar_' + str(params['index']) + '_ab_', vae_obj.encoder_inputs, vae_obj.get_encoder_logvar)\n",
    "        vae_obj.get_decoder().save(dump_loc + 'dec_' + str(params['index']) + \"_a.h5\")\n",
    "\n",
    "        return h.history.copy()\n",
    "\n",
    "\n",
    "    ## Hyperparameter (hp) search\n",
    "    # Get all combinations of hp\n",
    "    hp = [dict(zip(p, v)) for v in product(*p.values())]\n",
    "    # add an index to the hyperparameters\n",
    "    for h, idx in zip(hp, list(range(len(hp)))):\n",
    "        h.update({'index': idx})\n",
    "\n",
    "    hp_h = [] # list of histories\n",
    "\n",
    "    # Perform grid search\n",
    "    for params in hp:\n",
    "        h = hp_process([x_train_a, x_train_b], y_train, [x_test_a, x_test_b], y_test, params)\n",
    "        hp_h.append(h)\n",
    "\n",
    "\n",
    "    ## Create a pandas dataframe (df) and store it\n",
    "\n",
    "    # Store just everything into a folder\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    # Prefixes for history and for the full history as a list\n",
    "    h_prefix = 'h_'\n",
    "    h_list_prefix = 'list_'\n",
    "\n",
    "    # init hp keys\n",
    "    for k in hp[0].keys():\n",
    "        data[k] = []\n",
    "    # write hp keys\n",
    "    for params in hp:\n",
    "        for k in params.keys():\n",
    "            data[k].append(params[k])\n",
    "\n",
    "    # init history keys\n",
    "    for k in hp_h[0].keys():\n",
    "        data[h_prefix + k] = []\n",
    "        data[h_prefix + h_list_prefix + k] = []\n",
    "    # write history keys\n",
    "    for h in hp_h:\n",
    "        for k in h.keys():\n",
    "            data[h_prefix + h_list_prefix + k].append(h[k])\n",
    "    # write final history keys\n",
    "    for h in hp_h:\n",
    "        for k in h.keys():\n",
    "            data[h_prefix + k].append(h[k][-1])\n",
    "\n",
    "    # Create pandas dataframe and store it\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_hdf(dump_loc + 'history.h5', key='df')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-24627164",
   "language": "python",
   "display_name": "PyCharm (vae_tools)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 987,
   "position": {
    "height": "40px",
    "left": "1273.52px",
    "right": "20px",
    "top": "255.75px",
    "width": "600px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}