{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''This script demonstrates how to build a variational autoencoder with Keras.\n",
    "\n",
    " KL-Divergence\n",
    " \n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "from scipy.optimize import fmin\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import norm\n",
    "import sys\n",
    "\n",
    "\n",
    "# show the network\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def dkl_gauss(mu1, std1, mu2, std2):\n",
    "    # DKL(N(mu1, std1)||N(mu2, std2)) \n",
    "    return np.log(std2) - np.log(std1) + std1**2 / (2 * std2**2) + (mu1 - mu2)**2 / (2 * std2**2) - 1/2\n",
    "\n",
    "def dkl_gauss_opt(mu_std, mu1, std1):\n",
    "    # returns DKL(N(mu_std)||N(mu1, std1))\n",
    "    return dkl_gauss(mu_std[0], mu_std[1], mu1, std1)\n",
    "\n",
    "def dkl_gauss_opt2(mu_std, mu1, std1, mu2, std2):\n",
    "    # returns the sum DKL(N(mu_std)||N(mu1, std1)) + DKL(N(mu_std)||N(mu2, std2))\n",
    "    return dkl_gauss(mu_std[0], mu_std[1], mu1, std1) + dkl_gauss(mu_std[0], mu_std[1], mu2, std2)\n",
    "\n",
    "def norm_multi(x, pdf):\n",
    "    # returns normalized PDF of all scipy continues random variable (rv_continuous) PDF functions in the list pdf\n",
    "    r = np.zeros(pdf[0].pdf(x).shape)\n",
    "    for f in pdf:\n",
    "        r += f.pdf(x)\n",
    "    return r / np.float(len(pdf))\n",
    "\n",
    "def entropy_sanity(pk, qk):\n",
    "    # Calculate KL divergence between the ground-truth pk and approximator qk\n",
    "    # remove zero values because it causes divide by zero errors: https://datascience.stackexchange.com/questions/11320/kl-divergence-returns-infinity\n",
    "    pk[pk <= sys.float_info.min] = sys.float_info.min\n",
    "    qk[qk <= sys.float_info.min] = sys.float_info.min\n",
    "    return entropy(pk = pk, qk = qk)\n",
    "\n",
    "\n",
    "def entropy_opt(mu_std, qk, x):\n",
    "    # returns DKL(N(mu_std)||qk)\n",
    "    rv = norm(loc = mu_std[0], scale = mu_std[1])\n",
    "    return entropy_sanity(pk = rv.pdf(x), qk = qk)\n",
    "\n",
    "\n",
    "# def entropy_opt2(mu_std, mu1 = mu1, std1 = std1, mu2 = mu2, std2 = std2):\n",
    "\n",
    "#initialize a normal distribution with frozen in mean=-1, std. dev.= 1\n",
    "rv = norm(loc = 0., scale = 1.0)\n",
    "rv1 = norm(loc = -3., scale = 1.0)\n",
    "rv2 = norm(loc = 3., scale = 1.0)\n",
    "x = np.arange(-10, 10, .001)\n",
    "\n",
    "# plot the pdfs of these normal distributions \n",
    "#plt.plot(x, rv.pdf(x), x, norm_multi(x, [rv1, rv2]))\n",
    "#plt.show()\n",
    "\n",
    "print(\"------------ Sanity check start -------------\")\n",
    "print(\"Calculate KL divergence between uni-modal distributions.\")\n",
    "print(\"Numeric values should be somehow equal to the analytic values!\")\n",
    "# numeric\n",
    "dkl_num_LvsM = entropy_sanity(pk = rv.pdf(x), qk = rv1.pdf(x))\n",
    "dkl_num_RvsM = entropy_sanity(pk = rv.pdf(x), qk = rv2.pdf(x))\n",
    "print(\"KLD left vs. middle (numeric): \" + str(dkl_num_LvsM))\n",
    "print(\"KLD right vs. middle (numeric): \" + str(dkl_num_RvsM))\n",
    "# analytic\n",
    "print(\"KLD left vs. middle (analytic): \" + str(dkl_gauss_opt([rv.mean(), rv.std()], mu1 = rv1.mean(), std1 = rv1.std())))\n",
    "print(\"KLD right vs. middle (analytic): \" + str(dkl_gauss_opt([rv.mean(), rv.std()], mu1 = rv1.mean(), std1 = rv2.std())))\n",
    "print(\"------------ Sanity check end -------------\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"A DK-minimization approach that samples either N_1 or N_2 will try to put the\" +\n",
    "    \"resulting N into the middle of both distributions, because it does not see any relation\" +\n",
    "    \"between these distributions\" +\n",
    "    \"This is because the KL defined on single Gaussian does only cover one Gaussian at a time,\" +\n",
    "    \"Therefore, it is obvious that the minimum resulting N has mu=0.5(mu_1 + mu_2) and std = sqrt(std_1^2 + std_2^2)\")\n",
    "print(\"\")\n",
    "\n",
    "def KLD_eval(rv, rv1, rv2):\n",
    "    print(\"mu_mid, mu_left, mu_right, std_mid, std_left, std_right: \"\n",
    "          + str(rv.mean()) + \", \" + str(rv1.mean()) + \", \" + str(rv2.mean()) + \", \"\n",
    "          + str(rv.std()) + \", \" + str(rv1.std()) + \", \" + str(rv2.std()))\n",
    "    print(\"1. KLD joint vs. middle (sum of single KLD, i.e. optimized by the NN): \" + str(dkl_gauss_opt2([rv.mean(), rv.std()], mu1 = rv1.mean(), std1 = rv1.std(),mu2 = rv2.mean(), std2 = rv2.std())))\n",
    "    print(\"2. KLD joint vs. middle (numeric and corrent KLD between joint and middle PDF): \" + str(entropy_sanity(pk = rv.pdf(x), qk = norm_multi(x, [rv1, rv2]))))\n",
    "    #plot the pdfs of these normal distributions \n",
    "    plt.plot(x, rv.pdf(x), x, norm_multi(x, [rv1, rv2]))\n",
    "    plt.show()\n",
    "    print(\"\")\n",
    "\n",
    "rv = norm(loc = 0., scale = .5)\n",
    "KLD_eval(rv, rv1, rv2)\n",
    "rv = norm(loc = 0., scale = 1.0)\n",
    "KLD_eval(rv, rv1, rv2)\n",
    "rv = norm(loc = 0., scale = 2.0)\n",
    "KLD_eval(rv, rv1, rv2)\n",
    "rv = norm(loc = 0., scale = 3.0)\n",
    "KLD_eval(rv, rv1, rv2)\n",
    "rv = norm(loc = 0., scale = 4.0)\n",
    "KLD_eval(rv, rv1, rv2)\n",
    "\n",
    "print(\"It can be seen, that both error functions (not metrics) are convex but\" + \n",
    "      \" at different points of the standard deviation. The KLD of the full joint\" + \n",
    "      \" dstiribution tries tends to have a minima where the overlapping is the highest\" + \n",
    "      \" whilst the disjunct approach only matches the variance quantity and average mean.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Sanity Check: Single Gaussian optimization\n",
    "\n",
    "# Inital seed\n",
    "mu_init = 1\n",
    "std_init = 1.2\n",
    "x0 = [mu_init, std_init]\n",
    "# Target parameters\n",
    "mu1 = 0.\n",
    "std1 = 1.\n",
    "\n",
    "xOpt = fmin(dkl_gauss_opt, x0, args=(mu1, std1,))\n",
    "print(\"Befor Optimization: \" + str(dkl_gauss_opt(x0, mu1 = mu1, std1 = std1)))\n",
    "print(\"After Optimization: \" + str(dkl_gauss_opt(xOpt, mu1 = mu1, std1 = std1)))\n",
    "print(\"mu: \" + str(round(xOpt[0],2)) + \", std: \" + str(round(xOpt[1],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Multi Gaussian optimization (as it would be done by NN)\n",
    "\n",
    "# Inital seed\n",
    "mu_init = 1\n",
    "var_init = 1.2\n",
    "x0 = [mu_init, var_init]\n",
    "# Target parameters\n",
    "mu1 = 1.\n",
    "std1 = 1.\n",
    "mu2 = -1.\n",
    "std2 = 1.\n",
    "\n",
    "#mu1 = 10.\n",
    "#std1 = 1.\n",
    "#mu2 = -10.\n",
    "#std2 = 2.\n",
    "\n",
    "\n",
    "xOpt = fmin(dkl_gauss_opt2, x0, args=(mu1, std1, mu2, std2,))\n",
    "print(\"Befor Optimization: \" + str(dkl_gauss_opt2(x0, mu1 = mu1, std1 = std1, mu2 = mu2, std2 = std2)))\n",
    "print(\"After Optimization: \" + str(dkl_gauss_opt2(xOpt, mu1 = mu1, std1 = std1, mu2 = mu2, std2 = std2)))\n",
    "print(\"mu: \" + str(round(xOpt[0],2)) + \", std: \" + str(round(xOpt[1],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Mixture of Gaussian optimization\n",
    "\n",
    "# Inital seed\n",
    "mu_init = 1\n",
    "var_init = 1.2\n",
    "x0 = [mu_init, var_init]\n",
    "# Target parameters\n",
    "mu1 = 1.\n",
    "var1 = 1.\n",
    "mu2 = -1.\n",
    "var2 = 1.\n",
    "\n",
    "pdf_joint = norm_multi(x, [norm(loc = mu1, scale = std1), norm(loc = mu2, scale = std2)])\n",
    "x = np.arange(-10, 10, .001)\n",
    "xOpt = fmin(entropy_opt, x0, args=(pdf_joint, x,))\n",
    "\n",
    "# Print\n",
    "rv = norm(loc = x0[0], scale = x0[1])\n",
    "print(\"Befor Optimization: \" + str(entropy_sanity(pk = rv.pdf(x), qk = pdf_joint)))\n",
    "rv = norm(loc = xOpt[0], scale = xOpt[1])\n",
    "print(\"After Optimization: \" + str(entropy_sanity(pk = rv.pdf(x), qk = pdf_joint)))\n",
    "print(\"mu: \" + str(round(xOpt[0],2)) + \", std: \" + str(round(xOpt[1],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "input_dim_dummy = 1 # const.\n",
    "latent_dim_1 = 16\n",
    "latent_dim_2 = 16\n",
    "latent_dim_samp = 1\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    mu, std = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(mu)[0], latent_dim_samp), mean=0., stddev=1.)\n",
    "    return mu + std * epsilon\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    mean_target = Input(shape=(latent_dim_samp,), name=\"input_mean_target\")\n",
    "    std_target = Input(shape=(latent_dim_samp,), name=\"input_std_target\")\n",
    "    x = Input(shape=(input_dim_dummy,), name=\"input_dummy\")\n",
    "    h1 = Dense(latent_dim_1, activation='relu', name=\"latent1\")(x)\n",
    "    h2 = Dense(latent_dim_2, activation='relu', name=\"latent2\")(h1)\n",
    "    mean = Dense(latent_dim_samp, name=\"mean\")(h2)\n",
    "    std = Dense(latent_dim_samp, activation='relu', name=\"std\")(h2)\n",
    "    \n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    sample        = Lambda(sampling, output_shape=(latent_dim_samp,), name=\"sample\")([mean, std])\n",
    "    sample_target = Lambda(sampling, output_shape=(latent_dim_samp,), name=\"sample_target\")([mean_target, std_target])\n",
    "    return sample, sample_target, mean_target, std_target, mean, std, x\n",
    "\n",
    "# Custom loss layer with KL and MSE loss\n",
    "class CustomVariationalReconstructLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalReconstructLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def kl_loss(self, mean_target, std_target, mean, std):\n",
    "        return K.sum(K.log(std_target) - K.log(std) + \n",
    "                     (K.square(std) + K.square(mean - mean_target)) / (2*K.square(std_target + K.epsilon())) -\n",
    "                     1/2. , axis=-1)\n",
    "\n",
    "    def kl_loss_n(self, mean_target, std_target):\n",
    "        return K.sum(K.log(std_target) + \n",
    "                     (1. + K.square(mean_target)) / (2*K.square(std_target + K.epsilon())) -\n",
    "                     1/2. , axis=-1)\n",
    "#    def kl_loss(self, mean_target, std_target, mean, std):\n",
    "#        return K.sum(- .5 * (1  + K.log(std) - K.log(std_target) - (K.square(std) + K.square(mean - mean_target)) / (K.square(std_target))), axis=-1)\n",
    "\n",
    "    def loss(self, sample, sample_target, mean_target, std_target, mean, std):\n",
    "        #mse_loss = latent_dim_samp * metrics.mean_squared_error(sample, sample_target)\n",
    "        mse_loss = 1 / (1+ K.abs(sample)) # try to learn an ambiguous variable (aka 0.) \n",
    "        return K.mean(mse_loss + self.kl_loss(mean_target, std_target, mean, std) + self.kl_loss_n(mean, std))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        sample = inputs[0]\n",
    "        sample_target = inputs[1]\n",
    "        mean_target = inputs[2]\n",
    "        std_target = inputs[3]\n",
    "        mean = inputs[4]\n",
    "        std = inputs[5]\n",
    "        loss = self.loss(sample, sample_target, mean_target, std_target, mean, std)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return sample\n",
    "\n",
    "# Custom loss layer with KL loss only\n",
    "class CustomVariationalLayer(CustomVariationalReconstructLayer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def loss(self, sample, sample_target, mean_target, std_target, mean, std):\n",
    "        return K.mean(self.kl_loss(mean_target, std_target, mean, std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define training data\n",
    "num = 5000\n",
    "mean_target_noise_std = 0.01\n",
    "mean_targets_gt = np.array([-1., 1.])\n",
    "\n",
    "std_target_noise_std = 0.01\n",
    "std_targets_gt = np.array([.1, .1])\n",
    "\n",
    "target_choice = np.random.choice(np.int32([0, 1]), size=(num,))\n",
    "mean_targets_samples = mean_targets_gt[target_choice] + mean_target_noise_std * np.random.standard_normal(size=(num,))\n",
    "std_target_samples = std_targets_gt[target_choice] + std_target_noise_std * (np.random.rand(num) - 0.5)\n",
    "#dummy = np.random.rand(num)\n",
    "#dummy = np.ones((num,))\n",
    "dummy = np.zeros((num,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sample, sample_target, mean_target, std_target, mean, std, x = get_model()\n",
    "y_kl = CustomVariationalLayer()([sample, sample_target, mean_target, std_target, mean, std])\n",
    "model_kl = Model([mean_target, std_target, x], y_kl)\n",
    "model_kl.compile(optimizer='adam', loss=None)\n",
    "\n",
    "SVG(model_to_dot(model_kl).create(prog='dot', format='svg'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "model_kl.fit(x = [mean_targets_samples, std_target_samples, dummy],\n",
    "            y = None,\n",
    "            shuffle=True,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "estimation_kl = Model(x, [mean, std])\n",
    "mean_estimated_kl, std_estimated_kl = estimation_kl.predict(np.array([0.5]))[0], estimation_kl.predict(np.array([0.5]))[1]\n",
    "print(\"mean_estimated: \" + str(mean_estimated_kl))\n",
    "print(\"std_estimated: \" + str(std_estimated_kl))\n",
    "# SVG(model_to_dot(estimation).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sample, sample_target, mean_target, std_target, mean, std, x = get_model()\n",
    "y_kle = CustomVariationalReconstructLayer()([sample, sample_target, mean_target, std_target, mean, std])\n",
    "model_kle = Model([mean_target, std_target, x], y_kle)\n",
    "model_kle.compile(optimizer='adam', loss=None)\n",
    "\n",
    "# train the model\n",
    "model_kle.fit(x = [mean_targets_samples, std_target_samples, dummy],\n",
    "            y = None,\n",
    "            shuffle=True,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "estimation_kle = Model(x, [mean, std])\n",
    "mean_estimated_kle, std_estimated_kle = estimation_kle.predict(np.array([0.5]))[0], estimation_kle.predict(np.array([0.5]))[1]\n",
    "print(\"mean_estimated: \" + str(mean_estimated_kle))\n",
    "print(\"std_estimated: \" + str(std_estimated_kle))\n",
    "# SVG(model_to_dot(estimation).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-cddc943f",
   "language": "python",
   "display_name": "PyCharm (notebooks)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}